#/usr/bin/env python
#-*-coding=utf-8-*-

'''
Created on Nov 15, 2015
@author :wangzhen
'''
try :
    import sys
    import scrapy
    from scrapy import Request
    #from weibo.items import WeiboItem
    from items import WeiboItem
    import os
    import sys
    import urllib
    import urllib2
    import cookielib
    import base64
    import re
    import hashlib
    import json
    import rsa
    import binascii
    import time
    import uniout   
    from StringIO import StringIO
    from lxml import etree
    import xml.etree.ElementTree as ET
    from random import randint
except ImportError:
        print >> sys.stderr, """\

There was a problem importing one of the Python modules required.
The error leading to this problem was:

%s

Please install a package which provides this module, or
verify that the module is installed correctly.

It's possible that the above module doesn't match the current version of Python,
which is:

%s

""" % (sys.exc_info(), sys.version)
        sys.exit(1)


def login(cookie_file):
    if os.path.exists(cookie_file):
        try :
            cookie_jar  = cookielib.LWPCookieJar(cookie_file)
            cookie_jar.load(ignore_discard=True, ignore_expires=True)
            loaded = 1
        except cookielib.LoadError:
            loaded = 0
            print 'Loading cookies error'
        
        #install loaded cookies for urllib2
        if loaded:
            cookie_support = urllib2.HTTPCookieProcessor(cookie_jar)
            opener         = urllib2.build_opener(cookie_support, urllib2.HTTPHandler)
            urllib2.install_opener(opener)
            print 'Loading cookies success'
            return 1
        else:
            print 'Loading cookie failed'
    else:
        print 'no cookie file'


class BreakingnewsSpider(scrapy.Spider):
    download_delay = 5
    cookie_file_name = 'cookie'
    #login via cookie file, cookie file was generated by weibo_login program.
    login(cookie_file_name)
    name = 'breakingnews'
    allowed_domains = ['weibo.com']
    start_urls = [
            "http://weibo.com/breakingnews",
            #"http://weibo.com/nbweekly",
            #"http://weibo.com/weitianxia",
            #"http://weibo.com/yangtse",
            #"http://weibo.com/chinadailywebsite",
            #"http://weibo.com/cctvxinwen"
            #"http://weibo.com/rmrb",
            ]
    headers = {'User-Agent' :'Mozilla/5.0 (X11; Linux i686; rv:8.0) Gecko/20100101 Firefox/8.0'}
    #cookie = {'ALF':'1479266440','SSOLoginState':'1447730440','SUB':'_2A257Tu1YDeTxGedI7FYV8CnKzTiIHXVYOlmQrDV8PUNbuNBeLWOkkW9VSBYBSionsTX9xOvaWrMywe5qOg..', 'SUBP':'0033WrSXqPxfM725Ws9jqgMF55529P9D9WFG.6mTiZ5eCg73__UvKbkO5JpX5K2t','SUE' :"es%3D24e597a5e50e9b9244284b2c67c7bc95%26ev%3Dv1%26es2%3Dd9bcaf78d2ce86577f79a46c9e4327d5%26rs0%3DSiibEdVM3eSPtSS%252FtBLXaXAufSegqAMDE80o%252BcJK0mDpdjgEkqxBmwjkTXeaIqf5jp%252ByR42QXNe1I3UWKTKDyecM%252FadMDDyLZrnxw18eopq1PaQm4Xur9Kh00NU6DNU9MstDRD1DKbCumE8AhfdOkdJMLkJDxzo%252F6KEgJFihmEs%253D%26rv%3D0",'SUHB':08JK0eEm4_w6VD ,'SUP':"cv%3D1%26bt%3D1447730440%26et%3D1447816840%26d%3Dc909%26i%3D0cdd%26us%3D1%26vf%3D0%26vt%3D0%26ac%3D0%26st%3D0%26uid%3D1674407664%26name%3Dwangzhen11aaa%2540163.com%26nick%3Dwaterblue%26fmp%3D%26lcp%3D2015-09-17%252017%253A29%253A09",'SUS':"SID-1674407664-1447730440-GZ-ncgr3-be28e683700831e206eda607c6220cdd"}
    
    #cookie = {'SUS':'SID-1674407664-1447730440-GZ-ncgr3-be28e683700831e206eda607c6220cdd','path':'/','domain':'.weibo.com'}
    #cookie = {'ALF':'1eba44dbebf62c27ae66e16d40e02964','path':'/','domain':'.weibo.com'}
    #cookie = {'YF-Ugrow-G0':'1eba44dbebf62c27ae66e16d40e02964','path':'/','domain':'weibo.com'}
    #cookie = {'sso_info':'v02m6alo5qztbeYl5GlnKaJrJ2WlKadlqWkj5OEto2zkLSMg5y2jaOQwA==','path':'/','domain':'.sina.com.cn'}
    def list_max_length_str_index(self,str_list):
        max_length_str_index =  0
        max_length = -1
        for i in xrange(len(str_list)):
            if len(str_list[i]) > max_length:
                max_length_str_index = i
                max_length = len(str_list[i])
        return max_length_str_index
    
    
    
    def page_parser(self, page, events_list, unique_id_list, events_url_list, events_title_list, events_time_list, events_img_list, events_refer_num_list, events_comment_num_list, events_like_num_list,dynamic = False):
        p_parser = etree.HTMLParser()
        if dynamic == False:
            tree = etree.parse(StringIO(page), p_parser)
            nodes = tree.xpath('.//script')
            for node in nodes:
                ahtml = etree.tostring(node,encoding="utf-8" ,method="text")
                if ahtml.startswith("FM.view") and "帮上头条" in ahtml:
                    s = re.search("{.*}", ahtml).group(0)
                    json_event_data = json.loads(s)
                    html = json_event_data["html"]

                    html = html.replace("\\n","").replace("\\t","").replace("\\","").replace("\\r", "")
                    events_parser = etree.HTMLParser()
                    events_tree = etree.parse(StringIO(html), events_parser)
                    events_detail = events_tree.xpath('.//div[@class="WB_detail"]')
                    events_node = events_tree.xpath('.//div[@class="WB_text W_f14"]')
                    event_idx = 0
                    for event_node in events_node:
                        texts_list = event_node.xpath(".//text()")
                        index = self.list_max_length_str_index(texts_list)
                        try:
                            # First Check if the news in weibo is complelte, if not ,it will throw a exception and continue.
                            mid_node = event_node.xpath(".//a [@suda-uatrack]")
                            events_url_list.append(mid_node[0].attrib['href'])
                            events_title_list.append(mid_node[0].attrib['title'])
                            unique_id_list.append(mid_node[0].attrib["suda-uatrack"].split(":")[1])
                            # Get weibo date
                            event_achievement_list = events_detail[event_idx].getparent().getparent().xpath('.//div [@class = "WB_handle"]')[0].xpath('.//li')

                            events_refer_num_list.append("".join(event_achievement_list[1].itertext()).strip().split(' ')[1])
                            events_comment_num_list.append("".join(event_achievement_list[2].itertext()).strip().split(' ')[1])
                            events_like_num_list.append("".join(event_achievement_list[3].itertext()).strip())
                            time_node = events_detail[event_idx].xpath(".//a [@date][@name]")
                            events_time_list.append(time_node[0].attrib['title'])
                            # Get weibo img href of the events

                            events_img_tag_list = event_node.getparent().xpath(".//img [@src]")
                            event_img_url = {}
                            tmp_events_img_list = []
                            for img_idx in xrange(0, len(events_img_tag_list)):
                                tmp_events_img_list.append(events_img_tag_list[img_idx].attrib['src'].replace("square", "bmiddle"))
                            event_img_url['event_url'] = tmp_events_img_list
                            events_img_list.append(event_img_url)
                            events_list.append(texts_list[index].strip())
                        except Exception, e:
                            print 'news is not complete'
                            print e
                            continue
                        finally:
                            event_idx = event_idx + 1

        else:
            events_tree = etree.parse(StringIO(page), p_parser)
            events_detail = events_tree.xpath('.//div[@class="WB_detail"]')
            events_node = events_tree.xpath('.//div[@class="WB_text W_f14"]')
            event_idx = 0
            for event_node in events_node:
                print 100 * '='
                texts_list = event_node.xpath(".//text()")
                index = self.list_max_length_str_index(texts_list)
                try:
                    mid_node = event_node.xpath(".//a [@suda-uatrack]")
                    events_url_list.append(mid_node[0].attrib['href'])
                    events_title_list.append(mid_node[0].attrib['title'])
                    unique_id_list.append(mid_node[0].attrib["suda-uatrack"].split(":")[1])
                    event_achievement_list = events_detail[event_idx].getparent().getparent().xpath('.//div [@class = "WB_handle"]')[0].xpath('.//li')
                    events_refer_num_list.append("".join(event_achievement_list[1].itertext()).strip().split(' ')[1])
                    events_comment_num_list.append("".join(event_achievement_list[2].itertext()).strip().split(' ')[1])
                    events_like_num_list.append("".join(event_achievement_list[3].itertext()).strip())

                    events_img_tag_list = event_node.getparent().xpath(".//img [@src]")
                    event_img_url = {}
                    tmp_events_img_list = []
                    for img_idx in xrange(0, len(events_img_tag_list)):
                        tmp_events_img_list.append(events_img_tag_list[img_idx].attrib['src'].replace("square", "bmiddle"))
                    
                    event_img_url['event_url'] = tmp_events_img_list
                    events_img_list.append(event_img_url)

                    time_node = events_detail[event_idx].xpath(".//a [@date]")
                    events_time_list.append(time_node[0].attrib['title'])
                    events_list.append(texts_list[index].strip())
                except Exception, e:
                    print 'news is not complete'
                    print e
                    continue
                finally:
                    event_idx = event_idx + 1
# We want to produce local timestamp, then replace dot with a random number from 0 t0 10.
    def get_local_ts(self):
        local_ts = str(time.time())
        random_num = randint(0, 9)
        local_ts = local_ts.replace(".", str(random_num))
        return local_ts
    
    def comments_parser(self, comment_page_json, comments_list, max_id = []):
        comment_parser = etree.HTMLParser()
        json_comment = json.loads(comment_page_json)
        comment_page_json =  json_comment['data']['html']
        tree = etree.parse(StringIO(comment_page_json), comment_parser)
        
        comment_id_path = tree.xpath('//div [@comment_id]')
        comment_id = comment_id_path[0].attrib['comment_id']
        max_id.append(comment_id)
        comment_path = tree.xpath('//div[@class="WB_text"]')
        status_index = 0
        try:
            for comment_xml in comment_path:
                comment_info_dict = {}
                user_comment = "".join(comment_xml.itertext())
                comment_info_dict.setdefault('user', user_comment.encode('utf-8').split('：')[0])
                comment_info_dict.setdefault('comment', user_comment.encode('utf-8').split('：')[1])
                like_status_node = comment_xml.getparent().xpath('//span [@node-type="like_status"]')
                if "".join(like_status_node[status_index].itertext()) == '':
                    comment_info_dict.setdefault('comment_like_num', '0')
                comment_info_dict.setdefault('comment_like_num', "".join(like_status_node[status_index].itertext()))
                comment_time_node = comment_xml.getparent().xpath('//div [@class = "WB_from S_txt2"]')
                comment_info_dict.setdefault('comment_time', "".join(comment_time_node[status_index].itertext()))
                comments_list.append(comment_info_dict)
                #print comment_info_dict
                status_index += 1
        except Exception, e:
            print e
    
        return comments_list
    
    def get_target(self, html_str, pattern):
        search_results_list = re.findall(pattern, html_str)
        return search_results_list
    
    def parse(self, response):
        for page_idx in xrange(1, 100):
            
            # These two list should in the same order , and have same number.
            # events list
            events_list = []
            # mid list
            unique_id_list = []
            # weibo event url list
            events_url_list = []
            # weibo event title list
            events_title_list = []
            # weibo events time list
            events_time_list = []
            # weibo_image list
            events_img_list = []
    
            # number of news refers, comments,  likes
            events_refer_num_list = []
            events_comment_num_list = []
            events_like_num_list = []
            ts = self.get_local_ts()

            page_url = "http://weibo.com/breakingnews?is_search=0&visible=0&is_tag=0&profile_ftype=1&page="+str(page_idx)+"#"+ts
            web_page = urllib2.urlopen(page_url).read()

            #We get the domain id via regular expression
            domain_search_pattern = "CONFIG\['domain'\].*'"
            domain_result = self.get_target(web_page, domain_search_pattern)
            domain_id =  domain_result[0].split('=')[1].replace("'","")
    
           # #We get page id via regular expression
            page_id_search_pattern = "CONFIG\['page_id'\].*'"
            page_id_result = self.get_target(web_page, page_id_search_pattern)
            page_id = page_id_result[0].split('=')[1].replace("'", "")
           # 
           # # Here we can not use set function, because it can change the events order ,we use "mid=" in the regular expression instead "mid="
            web_page = self.page_parser(web_page, events_list, unique_id_list, events_url_list, events_title_list, events_time_list, events_img_list, events_refer_num_list, events_comment_num_list, events_like_num_list, dynamic = False)
           #print web_page
    
            local_ts = self.get_local_ts()
            #http://weibo.com/p/aj/v6/mblog/mbloglist?ajwvr=6&domain=100206&nick=%E5%A4%B4%E6%9D%A1%E6%96%B0%E9%97%BB&pre_page=1&page=1&max_id=&end_id=3885415314331468&pagebar=0&filtered_min_id=&pl_name=Pl_Official_MyProfileFeed__26&id=1002061618051664&script_uri=/breakingnews&feed_type=0&domain_op=100206&__rnd=1442372461187
    
            p_parser = etree.HTMLParser()
            for i in xrange(0, 2):
                # We produce a random time to sleep to avoid ip address being killed.
                tm = randint(8, 12)
                time.sleep(tm)
                try:
                    end_id = unique_id_list[0]
                except Exception , e:
                    print e
                    continue
                print 100 * '"'
                dynamic_url = "http://weibo.com/p/aj/v6/mblog/mbloglist?ajwvr=6&domain=" + domain_id + "&page="+ str(page_idx) +"&pre_page="+ str(page_idx) + "&end_id=" + end_id +"&pagebar=" + str(i) + "&pl_name=Pl_Official_MyProfileFeed__26&id=" + page_id + "&script_uri=/p/breakingnews&feed_type=0&domain_op=" +domain_id + "&__rnd=" + local_ts
                print dynamic_url
                web_page = urllib2.urlopen(dynamic_url).read()
                try:
                    json_data =  json.loads(web_page)
                    dynamic_page = json_data['data']
                except Exception ,e:
                    continue
                web_page = self.page_parser(web_page, events_list, unique_id_list, events_url_list, events_title_list, events_time_list, events_img_list, events_refer_num_list, events_comment_num_list, events_like_num_list,dynamic = True)
            print unique_id_list
    
            idx = 0
            for mid in unique_id_list:
                try:
                    comments_list = []
                    local_ts  = self.get_local_ts()
                    news_detail = WeiboItem()
                    news_detail["news_time"]= events_time_list[idx]
                    news_detail["news_img_url"]= events_img_list[idx]['event_url']
                    news_detail["news_title"]= events_title_list[idx]
                    news_detail["news_url"]= events_url_list[idx]
                    news_detail["news_content"]= events_list[idx]
                    news_detail["mid"]= mid
                    news_detail["news_refer_num"]= events_refer_num_list[idx]
                    news_detail["news_comment_num"]= events_comment_num_list[idx]
                    news_detail["news_like_num"]= events_like_num_list[idx]
                    
                    time.sleep(tm)
                    hot_url = "http://weibo.com/aj/v6/comment/big?ajwvr=6&id=" + mid + "&filter=hot&__rnd=" + local_ts
                    #print hot_url
                    hot_comment_page = urllib2.urlopen(hot_url).read()
                    comment_id_list = []
                    self.comments_parser(hot_comment_page, comments_list, comment_id_list)
                    # comment_id is the max_id in the next page
                    #comment_search_pattern = 'comment_id='
                    comment_id = comment_id_list[0]
                    try:
                        #dynamic page url is :http://weibo.com/aj/v6/comment/big?ajwvr=6&id=3890949954616149&max_id=3890950567155642&filter=hot&page=2&__rnd=1443168204996
                        for comment_page_idx in xrange(2, 40):
                            time.sleep(tm)
                            dynamic_comment_page_url = "http://weibo.com/aj/v6/comment/big?ajwvr=6&id=" + mid + "&max_id=" + comment_id + "&filter=hot&page=" + str(comment_page_idx) +" &__rnd=" + local_ts
                            
                            print dynamic_comment_page_url
    
                            hot_comment_page = urllib2.urlopen(dynamic_comment_page_url).read()
                            self.comments_parser(hot_comment_page, comments_list)
                            print 100  * '$'
                            news_detail.setdefault("comments", comments_list)
                    except Exception, e:
                        print e
                        continue
                except Exception, e:
                    print e
                    continue
                finally:
                    idx = idx + 1
                    news_detail.setdefault("comments", comments_list)
                    print news_detail
                    yield news_detail
